{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Schema Scraper\n",
    "\n",
    "1. Install all required modules (cohere, langchain) from `requirements.txt` first before running using `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can help with testing in a number of ways: \\n\\n- **Automated Testing**: Langsmith can be used to automate various testing tasks. With its natural language processing capabilities, Langsmith can understand and generate human language. This means it can be trained to automatically generate test cases, write test scripts, and even simulate user interactions with an interface, reducing the need for manual effort in these areas. \\n\\n- **Test Case Generation**: Langsmith can assist in creating comprehensive test case scenarios. By inputting the requirements and desired functionality, Langsmith can suggest test cases that cover a wide range of possibilities, including edge cases and boundary conditions, ensuring more robust testing. \\n\\n- **Test Data Generation**: Langsmith can generate large volumes of realistic and varied test data. This is particularly useful for testing applications that handle user-generated content, as it can simulate diverse user inputs, helping to uncover potential issues with data handling, validation, and display. \\n\\n- **Natural Language Understanding**: Langsmith's ability to understand natural language can be leveraged to create user-friendly test scripts and documentation. Testers can describe the desired functionality or behavior in plain English, and Langsmith can interpret these instructions, generating the necessary test scripts, saving time and reducing potential misunderstandings. \\n\\n- **Bug Reporting and Tracking**: Langsmith can assist in streamlining the bug reporting and tracking process. Testers can describe the issue in natural language, and Langsmith can categorize, prioritize, and generate bug reports, ensuring that all relevant information is captured and organized effectively for developers to address. \\n\\n- **Test Summary and Reporting**: After test execution, Langsmith can analyze the results and generate comprehensive test summary reports. These reports can include statistics, trends, and natural language descriptions of the test outcomes, making it easier for stakeholders to understand the testing progress and results. \\n\\n- **User Acceptance Testing (UAT)**: Langsmith can facilitate UAT by generating test scripts that reflect real-world usage scenarios. By involving actual user language and scenarios, Langsmith can help ensure that the application meets user expectations and requirements. \\n\\nBy leveraging Langsmith's capabilities, testing can become more efficient, comprehensive, and user-centric, ultimately contributing to improved software quality.\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '0ffccbf5-9757-4a9a-8f48-1a7584bef9d3'}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '0ffccbf5-9757-4a9a-8f48-1a7584bef9d3'}, id='run-32d91627-dc70-4def-82e6-e79905f3a94e-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with documentation retreival of LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Loader\n",
    "\n",
    "This uses the class `WebBaseLoader` to load a LLM documentation via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://ai.google.dev/models/gemini\") # url if the model documentation\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Index into a vectorstore. This requires an embedding model (Cohere) and a vectorstore\n",
    "\n",
    "embeddings = CohereEmbeddings()\n",
    "\n",
    "# Use simple local vectorstore, FAISS, to ingest documents into a vectorstore\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a retrieval chain from vectorstore\n",
    "# Chain will take a question, look up relevant documents, then pass them with original question into Cohere\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model schema data for the Gemini 1.0 Pro model includes the following:\n",
      "\n",
      "- **Input**: Text\n",
      "- **Output**: Text\n",
      "- **Model size**: Gemini 1.0 Pro is described as balancing capability and efficiency.\n",
      "- **Model code**: models/gemini-1.0-pro\n",
      "- **Capabilities**: Generates text and can handle multi-turn conversational format. It can also handle zero, one, and few-shot tasks.\n",
      "- **Supported generation methods**: generateContent\n",
      "- **Input token limit**: 30720\n",
      "- **Output token limit**: 2048\n",
      "- **Rate limit**: 60 requests per minute\n",
      "- **Latest stable version**: gemini-1.0-pro\n",
      "\n",
      "This information is provided in the context you supplied, which appears to be a documentation page for the Gemini API and its various models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"what is the model schema data of Gemini 1.0 Pro\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
